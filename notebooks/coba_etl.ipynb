{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9365665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jupyter\n",
      "  Using cached jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Collecting minio\n",
      "  Using cached minio-7.2.16-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting qdrant-client\n",
      "  Using cached qdrant_client-1.15.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-0.3.30-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting luigi\n",
      "  Using cached luigi-3.6.0.tar.gz (1.2 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting langchain[docarray]\n",
      "  Using cached langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting notebook (from jupyter)\n",
      "  Using cached notebook-7.4.5-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jupyter-console (from jupyter)\n",
      "  Using cached jupyter_console-6.6.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting nbconvert (from jupyter)\n",
      "  Using cached nbconvert-7.16.6-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: ipykernel in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from jupyter) (6.30.1)\n",
      "Collecting ipywidgets (from jupyter)\n",
      "  Using cached ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting jupyterlab (from jupyter)\n",
      "  Using cached jupyterlab-4.4.6-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting argon2-cffi (from minio)\n",
      "  Using cached argon2_cffi-25.1.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting certifi (from minio)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting pycryptodome (from minio)\n",
      "  Using cached pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting typing-extensions (from minio)\n",
      "  Using cached typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting urllib3 (from minio)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting grpcio>=1.41.0 (from qdrant-client)\n",
      "  Downloading grpcio-1.74.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting httpx>=0.20.0 (from httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting portalocker<4.0,>=2.7.0 (from qdrant-client)\n",
      "  Using cached portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting protobuf>=3.20.0 (from qdrant-client)\n",
      "  Downloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 (from qdrant-client)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.74 (from langchain-openai)\n",
      "  Using cached langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting openai<2.0.0,>=1.99.9 (from langchain-openai)\n",
      "  Downloading openai-1.100.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Downloading tiktoken-0.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting langsmith>=0.3.45 (from langchain-core<1.0.0,>=0.3.74->langchain-openai)\n",
      "  Using cached langsmith-0.4.14-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.74->langchain-openai)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.74->langchain-openai)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain-core<1.0.0,>=0.3.74->langchain-openai)\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: packaging>=23.2 in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (25.0)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.74->langchain-openai)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai<2.0.0,>=1.99.9->langchain-openai)\n",
      "  Using cached anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.99.9->langchain-openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.99.9->langchain-openai)\n",
      "  Downloading jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai<2.0.0,>=1.99.9->langchain-openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai<2.0.0,>=1.99.9->langchain-openai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai<2.0.0,>=1.99.9->langchain-openai)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client)\n",
      "  Downloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain-openai)\n",
      "  Downloading regex-2025.7.34-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting requests>=2.26.0 (from tiktoken<1,>=0.7->langchain-openai)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "\u001b[33mWARNING: langchain 0.3.27 does not provide the extra 'docarray'\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain[docarray])\n",
      "  Using cached langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain[docarray])\n",
      "  Downloading sqlalchemy-2.0.43-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai)\n",
      "  Downloading charset_normalizer-3.4.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain[docarray])\n",
      "  Downloading greenlet-3.2.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.74->langchain-openai)\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: tornado<7,>=5.0 in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from luigi) (6.5.2)\n",
      "Collecting python-daemon (from luigi)\n",
      "  Using cached python_daemon-3.1.2-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached h2-4.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting hyperframe<7,>=6.1 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting hpack<5,>=4.1 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting orjson>=3.9.14 (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain-openai)\n",
      "  Downloading orjson-3.11.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain-openai)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain-openai)\n",
      "  Downloading zstandard-0.24.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.1 kB)\n",
      "Collecting argon2-cffi-bindings (from argon2-cffi->minio)\n",
      "  Using cached argon2_cffi_bindings-25.1.0-cp39-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting cffi>=1.0.1 (from argon2-cffi-bindings->argon2-cffi->minio)\n",
      "  Downloading cffi-1.17.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting pycparser (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from ipykernel->jupyter) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from ipykernel->jupyter) (1.8.16)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from ipykernel->jupyter) (9.4.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from ipykernel->jupyter) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from ipykernel->jupyter) (5.8.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from ipykernel->jupyter) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from ipykernel->jupyter) (1.6.0)\n",
      "Requirement already satisfied: psutil>=5.7 in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from ipykernel->jupyter) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=25 in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from ipykernel->jupyter) (27.0.1)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from ipykernel->jupyter) (5.14.3)\n",
      "Requirement already satisfied: decorator in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->jupyter) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.4)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.3.8)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter) (0.7.0)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets->jupyter)\n",
      "  Using cached widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets->jupyter)\n",
      "  Using cached jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter)\n",
      "  Using cached async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting jinja2>=3.0.3 (from jupyterlab->jupyter)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter)\n",
      "  Using cached jupyter_lsp-2.2.6-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter)\n",
      "  Using cached jupyter_server-2.16.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter)\n",
      "  Using cached jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting notebook-shim>=0.2 (from jupyterlab->jupyter)\n",
      "  Using cached notebook_shim-0.2.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting setuptools>=41.1.0 (from jupyterlab->jupyter)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting nbformat>=5.3.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting prometheus-client>=0.9 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached prometheus_client-0.22.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting send2trash>=1.8.2 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached Send2Trash-1.8.3-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting terminado>=0.8.3 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached terminado-0.18.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting websocket-client>=1.7 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting babel>=2.10 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
      "  Using cached babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
      "  Using cached json5-0.12.1-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting jsonschema>=4.18.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
      "  Downloading jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=3.0.3->jupyterlab->jupyter)\n",
      "  Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
      "  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
      "  Downloading rpds_py-0.27.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting rfc3987-syntax>=1.1.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached rfc3987_syntax-1.1.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting webcolors>=24.6.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached webcolors-24.11.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting beautifulsoup4 (from nbconvert->jupyter)\n",
      "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting bleach!=5.0.0 (from bleach[css]!=5.0.0->nbconvert->jupyter)\n",
      "  Using cached bleach-6.2.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting defusedxml (from nbconvert->jupyter)\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting jupyterlab-pygments (from nbconvert->jupyter)\n",
      "  Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting mistune<4,>=2.0.3 (from nbconvert->jupyter)\n",
      "  Using cached mistune-3.1.3-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting nbclient>=0.5.0 (from nbconvert->jupyter)\n",
      "  Using cached nbclient-0.10.2-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting pandocfilters>=1.4.1 (from nbconvert->jupyter)\n",
      "  Using cached pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting webencodings (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter)\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting tinycss2<1.5,>=1.1.0 (from bleach[css]!=5.0.0->nbconvert->jupyter)\n",
      "  Using cached tinycss2-1.4.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting fastjsonschema>=2.15 (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached fastjsonschema-2.21.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting lark>=1.2.2 (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->nbconvert->jupyter)\n",
      "  Using cached soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Using cached types_python_dateutil-2.9.0.20250809-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting lockfile>=0.10 (from python-daemon->luigi)\n",
      "  Using cached lockfile-0.12.2-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/ori/sentiment-project/autoUpdateData/.venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter) (0.2.3)\n",
      "Using cached jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading pandas-2.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached minio-7.2.16-py3-none-any.whl (95 kB)\n",
      "Using cached qdrant_client-1.15.1-py3-none-any.whl (337 kB)\n",
      "Using cached portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached langchain_openai-0.3.30-py3-none-any.whl (74 kB)\n",
      "Using cached langchain_core-0.3.74-py3-none-any.whl (443 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading openai-1.100.1-py3-none-any.whl (788 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m788.3/788.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached anyio-4.10.0-py3-none-any.whl (107 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Using cached langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_text_splitters-0.3.9-py3-none-any.whl (33 kB)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (151 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading sqlalchemy-2.0.43-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Downloading greenlet-3.2.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (607 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.6/607.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.74.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached h2-4.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached hpack-4.1.0-py3-none-any.whl (34 kB)\n",
      "Using cached hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached langsmith-0.4.14-py3-none-any.whl (373 kB)\n",
      "Downloading numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.11.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "Downloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m767.5/767.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.7.34-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.9/801.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading zstandard-0.24.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached argon2_cffi-25.1.0-py3-none-any.whl (14 kB)\n",
      "Using cached argon2_cffi_bindings-25.1.0-cp39-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (87 kB)\n",
      "Downloading cffi-1.17.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479 kB)\n",
      "Using cached ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Using cached jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Using cached widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "Using cached jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Using cached jupyterlab-4.4.6-py3-none-any.whl (12.3 MB)\n",
      "Using cached jupyter_server-2.16.0-py3-none-any.whl (386 kB)\n",
      "Using cached jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
      "Using cached async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
      "Using cached babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached json5-0.12.1-py3-none-any.whl (36 kB)\n",
      "Downloading jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Using cached jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
      "Using cached jupyter_lsp-2.2.6-py3-none-any.whl (69 kB)\n",
      "Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Using cached nbconvert-7.16.6-py3-none-any.whl (258 kB)\n",
      "Using cached mistune-3.1.3-py3-none-any.whl (53 kB)\n",
      "Using cached bleach-6.2.0-py3-none-any.whl (163 kB)\n",
      "Using cached tinycss2-1.4.0-py3-none-any.whl (26 kB)\n",
      "Using cached nbclient-0.10.2-py3-none-any.whl (25 kB)\n",
      "Using cached nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "Using cached fastjsonschema-2.21.2-py3-none-any.whl (24 kB)\n",
      "Using cached notebook_shim-0.2.4-py3-none-any.whl (13 kB)\n",
      "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Using cached pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Using cached prometheus_client-0.22.1-py3-none-any.whl (58 kB)\n",
      "Using cached python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
      "Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Using cached rfc3987_syntax-1.1.0-py3-none-any.whl (8.0 kB)\n",
      "Using cached lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "Downloading rpds_py-0.27.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (386 kB)\n",
      "Using cached Send2Trash-1.8.3-py3-none-any.whl (18 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached terminado-0.18.1-py3-none-any.whl (14 kB)\n",
      "Using cached webcolors-24.11.1-py3-none-any.whl (14 kB)\n",
      "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Using cached soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Using cached arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "Using cached types_python_dateutil-2.9.0.20250809-py3-none-any.whl (17 kB)\n",
      "Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
      "Using cached notebook-7.4.5-py3-none-any.whl (14.3 MB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Using cached pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "Using cached python_daemon-3.1.2-py3-none-any.whl (30 kB)\n",
      "Using cached lockfile-0.12.2-py2.py3-none-any.whl (13 kB)\n",
      "Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Using cached uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Building wheels for collected packages: luigi\n",
      "  Building wheel for luigi (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for luigi: filename=luigi-3.6.0-py3-none-any.whl size=1093838 sha256=c8ebd84de9c8e30cb77a09c55d5fb80b175e5c5ede2252944ff2ae970e6a086d\n",
      "  Stored in directory: /home/ori/.cache/pip/wheels/6b/67/54/c530d79c6eb27962863fe9f4d1612951ae346f7a6f541259f9\n",
      "Successfully built luigi\n",
      "Installing collected packages: webencodings, pytz, lockfile, fastjsonschema, zstandard, widgetsnbextension, websocket-client, webcolors, urllib3, uri-template, tzdata, typing-extensions, types-python-dateutil, tqdm, tinycss2, terminado, tenacity, soupsieve, sniffio, setuptools, send2trash, rpds-py, rfc3986-validator, rfc3339-validator, regex, PyYAML, python-json-logger, python-dotenv, python-daemon, pycryptodome, pycparser, protobuf, prometheus-client, portalocker, pandocfilters, overrides, orjson, numpy, mistune, MarkupSafe, lark, jupyterlab_widgets, jupyterlab-pygments, jsonpointer, json5, jiter, idna, hyperframe, hpack, h11, grpcio, greenlet, fqdn, distro, defusedxml, charset_normalizer, certifi, bleach, babel, attrs, async-lru, annotated-types, typing-inspection, SQLAlchemy, rfc3987-syntax, requests, referencing, pydantic-core, pandas, luigi, jupyter-server-terminals, jsonpatch, jinja2, httpcore, h2, cffi, beautifulsoup4, arrow, anyio, tiktoken, requests-toolbelt, pydantic, jsonschema-specifications, isoduration, ipywidgets, httpx, argon2-cffi-bindings, openai, langsmith, jupyter-console, jsonschema, argon2-cffi, qdrant-client, nbformat, minio, langchain-core, nbclient, langchain-text-splitters, langchain-openai, jupyter-events, nbconvert, langchain, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, jupyter\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109/109\u001b[0m [jupyter][notebook]jupyterlab]]erver]minals]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 PyYAML-6.0.2 SQLAlchemy-2.0.43 annotated-types-0.7.0 anyio-4.10.0 argon2-cffi-25.1.0 argon2-cffi-bindings-25.1.0 arrow-1.3.0 async-lru-2.0.5 attrs-25.3.0 babel-2.17.0 beautifulsoup4-4.13.4 bleach-6.2.0 certifi-2025.8.3 cffi-1.17.1 charset_normalizer-3.4.3 defusedxml-0.7.1 distro-1.9.0 fastjsonschema-2.21.2 fqdn-1.5.1 greenlet-3.2.4 grpcio-1.74.0 h11-0.16.0 h2-4.2.0 hpack-4.1.0 httpcore-1.0.9 httpx-0.28.1 hyperframe-6.1.0 idna-3.10 ipywidgets-8.1.7 isoduration-20.11.0 jinja2-3.1.6 jiter-0.10.0 json5-0.12.1 jsonpatch-1.33 jsonpointer-3.0.0 jsonschema-4.25.1 jsonschema-specifications-2025.4.1 jupyter-1.1.1 jupyter-console-6.6.3 jupyter-events-0.12.0 jupyter-lsp-2.2.6 jupyter-server-2.16.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.6 jupyterlab-pygments-0.3.0 jupyterlab-server-2.27.3 jupyterlab_widgets-3.0.15 langchain-0.3.27 langchain-core-0.3.74 langchain-openai-0.3.30 langchain-text-splitters-0.3.9 langsmith-0.4.14 lark-1.2.2 lockfile-0.12.2 luigi-3.6.0 minio-7.2.16 mistune-3.1.3 nbclient-0.10.2 nbconvert-7.16.6 nbformat-5.10.4 notebook-7.4.5 notebook-shim-0.2.4 numpy-2.3.2 openai-1.100.1 orjson-3.11.2 overrides-7.7.0 pandas-2.3.1 pandocfilters-1.5.1 portalocker-3.2.0 prometheus-client-0.22.1 protobuf-6.32.0 pycparser-2.22 pycryptodome-3.23.0 pydantic-2.11.7 pydantic-core-2.33.2 python-daemon-3.1.2 python-dotenv-1.1.1 python-json-logger-3.3.0 pytz-2025.2 qdrant-client-1.15.1 referencing-0.36.2 regex-2025.7.34 requests-2.32.5 requests-toolbelt-1.0.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rfc3987-syntax-1.1.0 rpds-py-0.27.0 send2trash-1.8.3 setuptools-80.9.0 sniffio-1.3.1 soupsieve-2.7 tenacity-8.5.0 terminado-0.18.1 tiktoken-0.11.0 tinycss2-1.4.0 tqdm-4.67.1 types-python-dateutil-2.9.0.20250809 typing-extensions-4.14.1 typing-inspection-0.4.1 tzdata-2025.2 uri-template-1.3.0 urllib3-2.5.0 webcolors-24.11.1 webencodings-0.5.1 websocket-client-1.8.0 widgetsnbextension-4.0.14 zstandard-0.24.0\n"
     ]
    }
   ],
   "source": [
    "# Jalankan sel ini untuk menginstal semua dependensi yang diperlukan\n",
    "! pip install jupyter python-dotenv pandas minio qdrant-client langchain-openai \"langchain[docarray]\" luigi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10e45456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: luigi==1.0.3 in /home/ori/sentiment-project/sentiment-project-twitter/lib/python3.13/site-packages (1.0.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install luigi==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee5a84e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datetime import date\n",
    "from minio import Minio\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qmodels\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import glob\n",
    "import pandas as pd\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import luigi\n",
    "import boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "17959b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env\")\n",
    "\n",
    "def get_minio_client():\n",
    "    \"\"\"Membuat klien Minio untuk digunakan oleh Target.\"\"\"\n",
    "    return Minio(\n",
    "        os.getenv(\"MINIO_ENDPOINT\"),\n",
    "        access_key=os.getenv(\"MINIO_ACCESS_KEY\"),\n",
    "        secret_key=os.getenv(\"MINIO_SECRET_KEY\"),\n",
    "        secure=False\n",
    "    )\n",
    "\n",
    "def get_qdrant_client(host=\"localhost\", port=6333):\n",
    "    return QdrantClient(host=host, port=port)\n",
    "\n",
    "def get_s3_client_for_luigi():\n",
    "    \"\"\"Membuat klien boto3 yang dikonfigurasi untuk MinIO.\"\"\"\n",
    "    return boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=f'http://{os.getenv(\"MINIO_ENDPOINT\")}',\n",
    "        aws_access_key_id=os.getenv(\"MINIO_ACCESS_KEY\"),\n",
    "        aws_secret_access_key=os.getenv(\"MINIO_SECRET_KEY\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd7442b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Twitter Auth Token\n",
    "\n",
    "twitter_auth_token = os.getenv(\"TWITTER_AUTH_TOKEN\") # change this auth token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d566a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/ori/sentiment-project/sentiment-project-twitter/lib/python3.13/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/ori/sentiment-project/sentiment-project-twitter/lib/python3.13/site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ori/sentiment-project/sentiment-project-twitter/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ori/sentiment-project/sentiment-project-twitter/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ori/sentiment-project/sentiment-project-twitter/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ori/sentiment-project/sentiment-project-twitter/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "[sudo] password for ori: sudo: a password is required\n",
      "^C\n",
      "[sudo] password for ori: "
     ]
    }
   ],
   "source": [
    "# Import required Python package\n",
    "!pip install pandas\n",
    "\n",
    "# Install Node.js (because tweet-harvest built using Node.js)\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y ca-certificates curl gnupg\n",
    "!sudo mkdir -p /etc/apt/keyrings\n",
    "!curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg\n",
    "\n",
    "!NODE_MAJOR=20 && echo \"deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main\" | sudo tee /etc/apt/sources.list.d/nodesource.list\n",
    "\n",
    "!sudo apt-get update\n",
    "\n",
    "!sudo apt-get install nodejs -y\n",
    "\n",
    "!node -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce6acbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting crawl for 100 tweets...\n",
      "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K\u001b[1m\u001b[32mTweet Harvest [v2.6.1]\u001b[39m\u001b[22m\n",
      "\u001b[1m\u001b[32m\u001b[39m\u001b[22m\n",
      "\u001b[34mResearch by \u001b[39m\u001b[1m\u001b[34mHelmi Satria\u001b[39m\u001b[22m\u001b[34m\u001b[39m\n",
      "\u001b[34mUse it for Educational Purposes only!\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[33mThis script uses Chromium Browser to crawl data from Twitter with \u001b[1myour Twitter auth token\u001b[22m.\u001b[39m\n",
      "\u001b[33mPlease enter your Twitter auth token when prompted.\u001b[39m\n",
      "\u001b[33m\u001b[39m\n",
      "\u001b[31m\u001b[1mNote:\u001b[22m\u001b[39m Keep your access token secret! Don't share it with anyone else.\n",
      "\u001b[31m\u001b[1mNote:\u001b[22m\u001b[39m This script only runs on your local device.\n",
      "\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mOpening twitter search page...\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[33m\u001b[39m\n",
      "\u001b[33mFilling in keywords: telkomsel since:2025-08-02 until:2025-08-10 lang:id\u001b[39m\n",
      "\u001b[33m\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mYour tweets saved to: /home/ori/sentiment-project/autoUpdateData/notebooks/tweets-data/gibran2.csv\u001b[39m\n",
      "\u001b[33mTotal tweets saved: 18\u001b[39m\n",
      "\u001b[90m\u001b[39m\n",
      "\u001b[90m-- Scrolling... (1)\u001b[39m\u001b[34m\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mYour tweets saved to: /home/ori/sentiment-project/autoUpdateData/notebooks/tweets-data/gibran2.csv\u001b[39m\n",
      "\u001b[33mTotal tweets saved: 32\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mYour tweets saved to: /home/ori/sentiment-project/autoUpdateData/notebooks/tweets-data/gibran2.csv\u001b[39m\n",
      "\u001b[33mTotal tweets saved: 43\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mYour tweets saved to: /home/ori/sentiment-project/autoUpdateData/notebooks/tweets-data/gibran2.csv\u001b[39m\n",
      "\u001b[33mTotal tweets saved: 57\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mYour tweets saved to: /home/ori/sentiment-project/autoUpdateData/notebooks/tweets-data/gibran2.csv\u001b[39m\n",
      "\u001b[33mTotal tweets saved: 72\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mYour tweets saved to: /home/ori/sentiment-project/autoUpdateData/notebooks/tweets-data/gibran2.csv\u001b[39m\n",
      "\u001b[33mTotal tweets saved: 91\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mYour tweets saved to: /home/ori/sentiment-project/autoUpdateData/notebooks/tweets-data/gibran2.csv\u001b[39m\n",
      "\u001b[33mTotal tweets saved: 111\u001b[39m\n",
      "\u001b[90m\u001b[39m\n",
      "\u001b[90m--Taking a break, waiting for 10 seconds...\u001b[39m\n",
      "Got 111 tweets, done scrolling...\n",
      "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0KCrawling finished. Data saved to gibran2.csv\n"
     ]
    }
   ],
   "source": [
    "# Crawl Data\n",
    "\n",
    "filename = 'gibran2.csv'\n",
    "search_keyword = 'telkomsel since:2025-08-02 until:2025-08-10 lang:id'\n",
    "# Tentukan limit yang Anda inginkan\n",
    "limit = 100\n",
    "\n",
    "# Pastikan twitter_auth_token sudah didefinisikan sebelumnya, misalnya:\n",
    "# twitter_auth_token = \"...\" \n",
    "\n",
    "print(f\"Starting crawl for {limit} tweets...\")\n",
    "\n",
    "# Gunakan variabel {limit}, bukan {10}\n",
    "! npx -y tweet-harvest@2.6.1 -o \"{filename}\" -s \"{search_keyword}\" --tab \"LATEST\" -l {limit} --token {twitter_auth_token}\n",
    "\n",
    "print(f\"Crawling finished. Data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "858cd724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import luigi\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Pastikan load_dotenv() dipanggil di atas\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# ==============================================================================\n",
    "# TUGAS 1: EKSTRAKSI TWEET (DENGAN POLA YANG BENAR)\n",
    "# ==============================================================================\n",
    "kemarin = date.today() - timedelta(days=1)\n",
    "\n",
    "class ExtractTweets(luigi.Task):\n",
    "    task_date = luigi.DateParameter(default=kemarin)\n",
    "\n",
    "    def output(self):\n",
    "        \"\"\"\n",
    "        Mendeklarasikan path akhir dari file output.\n",
    "        \"\"\"\n",
    "        # Dapatkan direktori root proyek\n",
    "        project_root = Path.cwd()\n",
    "        output_dir = project_root / \"tweets-data\"\n",
    "        \n",
    "        # Pastikan direktori ada\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Return LocalTarget dengan path absolut\n",
    "        absolute_path = output_dir / f\"{self.task_date.strftime('%Y-%m-%d')}.csv\"\n",
    "        return luigi.LocalTarget(absolute_path)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Bertanggung jawab untuk menjalankan scraping dan menyimpan hasilnya\n",
    "        ke path yang didefinisikan di self.output().\n",
    "        \"\"\"\n",
    "        print(f\"--- [Extract] Memulai ekstraksi data Twitter untuk {self.task_date} ---\")\n",
    "        \n",
    "        # 1. Siapkan semua parameter yang dibutuhkan\n",
    "        start_date = self.task_date.strftime('%Y-%m-%d')\n",
    "        end_date_formatted = datetime.now().strftime('%Y-%m-%d')\n",
    "        search_keyword = f'\"telkomsel\" since:{start_date} until:{end_date_formatted} lang:id'\n",
    "        limit = 10\n",
    "        twitter_auth_token = os.getenv('TWITTER_AUTH_TOKEN')\n",
    "        \n",
    "        # 2. Definisikan nama file sementara untuk hasil scraping\n",
    "        temp_filename = f\"temp_scrape_{self.task_date.strftime('%Y%m%d')}.csv\"\n",
    "\n",
    "        # 3. Jalankan perintah tweet-harvest untuk menyimpan ke file sementara\n",
    "        command = (\n",
    "            f'npx -y tweet-harvest@2.6.1 -o \"{temp_filename}\" '\n",
    "            f'-s \"{search_keyword}\" --tab \"LATEST\" -l {limit} '\n",
    "            f'--token \"{twitter_auth_token}\"'\n",
    "        )\n",
    "        print(f\"Menjalankan perintah: {command}\")\n",
    "        os.system(command)\n",
    "        project_root = Path.cwd()\n",
    "        print(temp_filename)\n",
    "        temp_filename=project_root/\"tweets-data\"/temp_filename\n",
    "        print(temp_filename)\n",
    "        # 4. Baca data dari file sementara dan simpan ke tujuan akhir (output)\n",
    "        if os.path.exists(temp_filename):\n",
    "            # Baca hasil scraping ke dalam DataFrame\n",
    "            scraped_data_df = pd.read_csv(temp_filename)\n",
    "            \n",
    "            # Simpan DataFrame ke path akhir yang didefinisikan oleh self.output()\n",
    "            # Ini sama persis dengan pola di contoh ScrapeData Anda\n",
    "            scraped_data_df.to_csv(self.output().path, index=False)\n",
    "            \n",
    "            # 5. Hapus file sementara setelah selesai\n",
    "            os.remove(temp_filename)\n",
    "            print(f\"--- [Extract] Selesai. Data disimpan di {self.output().path} ---\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Scraping gagal, file sementara tidak ditemukan: {temp_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32c0bb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Checking if ExtractTweets(task_date=2025-08-19) is complete\n",
      "INFO: Informed scheduler that task   ExtractTweets_2025_08_19_204f0f2397   has status   PENDING\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 1\n",
      "INFO: [pid 45361] Worker Worker(salt=7126721917, workers=1, host=LAPTOP-GCD4EMJB, username=ori, pid=45361) running   ExtractTweets(task_date=2025-08-19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Extract] Memulai ekstraksi data Twitter untuk 2025-08-19 ---\n",
      "Menjalankan perintah: npx -y tweet-harvest@2.6.1 -o \"temp_scrape_20250819.csv\" -s \"\"telkomsel\" since:2025-08-19 until:2025-08-20 lang:id\" --tab \"LATEST\" -l 10 --token \"c2c3b68dd8544bd3d22cdc0e48ec05846b17f129\"\n",
      "\u001b[1m\u001b[32mTweet Harvest [v2.6.1]\u001b[39m\u001b[22m\n",
      "\u001b[1m\u001b[32m\u001b[39m\u001b[22m\n",
      "\u001b[34mResearch by \u001b[39m\u001b[1m\u001b[34mHelmi Satria\u001b[39m\u001b[22m\u001b[34m\u001b[39m\n",
      "\u001b[34mUse it for Educational Purposes only!\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[33mThis script uses Chromium Browser to crawl data from Twitter with \u001b[1myour Twitter auth token\u001b[22m.\u001b[39m\n",
      "\u001b[33mPlease enter your Twitter auth token when prompted.\u001b[39m\n",
      "\u001b[33m\u001b[39m\n",
      "\u001b[31m\u001b[1mNote:\u001b[22m\u001b[39m Keep your access token secret! Don't share it with anyone else.\n",
      "\u001b[31m\u001b[1mNote:\u001b[22m\u001b[39m This script only runs on your local device.\n",
      "\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mOpening twitter search page...\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[90m\u001b[39m\n",
      "\u001b[90m-- Scrolling... (1)\u001b[39m\u001b[33m\u001b[39m\n",
      "\u001b[33mFilling in keywords: telkomsel since:2025-08-19 until:2025-08-20 lang:id\u001b[39m\n",
      "\u001b[33m\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mYour tweets saved to: /home/ori/sentiment-project/autoUpdateData/notebooks/tweets-data/temp_scrape_20250819.csv\u001b[39m\n",
      "\u001b[33mTotal tweets saved: 20\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: [pid 45361] Worker Worker(salt=7126721917, workers=1, host=LAPTOP-GCD4EMJB, username=ori, pid=45361) done      ExtractTweets(task_date=2025-08-19)\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   ExtractTweets_2025_08_19_204f0f2397   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "INFO: Worker Worker(salt=7126721917, workers=1, host=LAPTOP-GCD4EMJB, username=ori, pid=45361) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 1 tasks of which:\n",
      "* 1 ran successfully:\n",
      "    - 1 ExtractTweets(task_date=2025-08-19)\n",
      "\n",
      "This progress looks :) because there were no failed tasks or missing dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 20 tweets, done scrolling...\n",
      "temp_scrape_20250819.csv\n",
      "/home/ori/sentiment-project/autoUpdateData/notebooks/tweets-data/temp_scrape_20250819.csv\n",
      "--- [Extract] Selesai. Data disimpan di /home/ori/sentiment-project/autoUpdateData/notebooks/tweets-data/2025-08-19.csv ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luigi.build([ExtractTweets()], local_scheduler = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0043a0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root directory: /home/ori/sentiment-project/autoUpdateData/notebooks\n",
      "Absolute output path: /home/ori/sentiment-project/autoUpdateData/notebooks/data/raw\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path relatif ke folder output dari root proyek\n",
    "output_path_relative = \"data/raw\"\n",
    "\n",
    "# Gunakan os.getcwd() untuk mendapatkan direktori kerja saat ini (root proyek)\n",
    "project_root_dir = os.getcwd()\n",
    "\n",
    "# Gabungkan untuk mendapatkan path absolut yang andal\n",
    "absolute_output_path = os.path.join(project_root_dir, output_path_relative)\n",
    "\n",
    "print(f\"Project root directory: {project_root_dir}\")\n",
    "print(f\"Absolute output path: {absolute_output_path}\")\n",
    "\n",
    "# Sekarang Anda bisa menggunakan path absolut ini\n",
    "# os.makedirs(absolute_output_path, exist_ok=True)\n",
    "# if not os.path.exists(absolute_output_path):\n",
    "#    ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
